{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"kinship-detection-with-vgg16.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"Nxuqvy-V52sH","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGaM-Zkl609V","colab_type":"code","outputId":"963b6ec3-41ea-4945-c2ba-830aa33ff17d","executionInfo":{"status":"ok","timestamp":1561748412746,"user_tz":-330,"elapsed":2068,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pwd"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"QYZ60h_Y6wzY","colab_type":"code","colab":{}},"source":["root_path = 'gdrive/My Drive/capstone_project/input/recognizing-faces-in-the-wild'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"assEblMy5-4V","colab_type":"code","outputId":"7674e4ec-ade6-42ba-b079-387a20bd87bf","executionInfo":{"status":"ok","timestamp":1561748412752,"user_tz":-330,"elapsed":2050,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","\n","import os\n","print(os.listdir(root_path))\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":0,"outputs":[{"output_type":"stream","text":["['test', 'train', '.ipynb_checkpoints', 'kernel_submission.csv', 'Data Cleaning and Preprocessing.ipynb', '.DS_Store', 'sample_submission.csv', 'combinations.txt', 'Untitled.ipynb', 'train_relationships.csv', 'test.zip', 'train.zip', 'df_final.csv', 'kinship_detection_with_vgg16.ipynb', 'open-kimono-kinship-notebook-0-792lb.ipynb', 'kinship-detection-with-vgg16.ipynb']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ruWP3EfC52sT","colab_type":"text"},"source":["Necassary Imports"]},{"cell_type":"code","metadata":{"id":"HyQRhJEB52sU","colab_type":"code","outputId":"9bed58f2-b82f-418c-c8b2-c7d1cf76fd42","executionInfo":{"status":"ok","timestamp":1561748420626,"user_tz":-330,"elapsed":9901,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":339}},"source":["!pip install git+https://github.com/rcmalli/keras-vggface.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/rcmalli/keras-vggface.git\n","  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-zi3msh1v\n","  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-zi3msh1v\n","Requirement already satisfied (use --upgrade to upgrade): keras-vggface==0.5 from git+https://github.com/rcmalli/keras-vggface.git in /usr/local/lib/python3.6/dist-packages\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.16.4)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.3.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (2.8.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (4.3.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (2.2.4)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.12.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (3.13)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->keras-vggface==0.5) (0.46)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vggface==0.5) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vggface==0.5) (1.0.8)\n","Building wheels for collected packages: keras-vggface\n","  Building wheel for keras-vggface (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-f83fmjik/wheels/36/07/46/06c25ce8e9cd396dabe151ea1d8a2bc28dafcb11321c1f3a6d\n","Successfully built keras-vggface\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"y7mVHLq752sY","colab_type":"code","colab":{}},"source":["import h5py\n","from collections import defaultdict\n","from glob import glob\n","from random import choice, sample\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras_vggface.utils import preprocess_input\n","from keras_vggface.vggface import VGGFace"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-O3h987L52sc","colab_type":"code","outputId":"82b42359-9b47-4e06-9454-862cd9fc2aa0","executionInfo":{"status":"ok","timestamp":1561748420631,"user_tz":-330,"elapsed":9866,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_file_path = root_path + \"/train_relationships.csv\"\n","train_folders_path = root_path + \"/train/\"\n","val_famillies = \"F09\"\n","train_folders_path"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'gdrive/My Drive/capstone_project/input/recognizing-faces-in-the-wild/train/'"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"a3FW_UHxQ27o","colab_type":"code","outputId":"5aaa0826-7ff5-4753-859f-8fb5638ed349","executionInfo":{"status":"ok","timestamp":1561748420632,"user_tz":-330,"elapsed":9843,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_file_path"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'gdrive/My Drive/capstone_project/input/recognizing-faces-in-the-wild/train_relationships.csv'"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"2bY426b-7ZbO","colab_type":"code","outputId":"a3223232-f0df-4626-f73e-119ca2f490b6","executionInfo":{"status":"ok","timestamp":1561748420633,"user_tz":-330,"elapsed":9826,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_folders_path"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'gdrive/My Drive/capstone_project/input/recognizing-faces-in-the-wild/train/'"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"TnXdnYFU52sg","colab_type":"code","outputId":"74c582b0-4da8-452d-b6a1-ce6c481e0d04","executionInfo":{"status":"ok","timestamp":1561748423361,"user_tz":-330,"elapsed":12533,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["all_images = glob(root_path + \"/train/\" + \"*/*/*.jpg\")\n","len(all_images)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12429"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"TbEt5mf552si","colab_type":"text"},"source":["Validation set consists only family 9"]},{"cell_type":"code","metadata":{"id":"6YOH0brK52si","colab_type":"code","colab":{}},"source":["train_images = [x for x in all_images if val_famillies not in x]\n","val_images = [x for x in all_images if val_famillies in x]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UGch8Pj52sk","colab_type":"code","colab":{}},"source":["train_person_to_images_map = defaultdict(list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4Z3fdcm52sn","colab_type":"code","colab":{}},"source":["ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwnc_c2552sq","colab_type":"code","colab":{}},"source":["for x in train_images:\n","    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n","\n","val_person_to_images_map = defaultdict(list)\n","\n","for x in val_images:\n","    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n","\n","relationships = pd.read_csv(train_file_path)\n","relationships = list(zip(relationships.p1.values, relationships.p2.values))\n","relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n","\n","train = [x for x in relationships if val_famillies not in x[0]]\n","val = [x for x in relationships if val_famillies in x[0]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUOHAglC52su","colab_type":"code","outputId":"b021c8eb-d2ac-4023-db7f-4d2dc36fcf80","executionInfo":{"status":"ok","timestamp":1561748444756,"user_tz":-330,"elapsed":33903,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":672}},"source":["def read_img(path):\n","    img = cv2.imread(path)\n","    img = np.array(img).astype(np.float)\n","    return preprocess_input(img, version=2)\n","\n","\n","def gen(list_tuples, person_to_images_map, batch_size=16):\n","    ppl = list(person_to_images_map.keys())\n","    while True:\n","        batch_tuples = sample(list_tuples, batch_size // 2)\n","        labels = [1] * len(batch_tuples)\n","        while len(batch_tuples) < batch_size:\n","            p1 = choice(ppl)\n","            p2 = choice(ppl)\n","\n","            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n","                batch_tuples.append((p1, p2))\n","                labels.append(0)\n","\n","        for x in batch_tuples:\n","            if not len(person_to_images_map[x[0]]):\n","                print(x[0])\n","\n","        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n","        X1 = np.array([read_img(x) for x in X1])\n","\n","        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n","        X2 = np.array([read_img(x) for x in X2])\n","\n","        yield [X1, X2], labels\n","\n","\n","def baseline_model():\n","    input_1 = Input(shape=(224, 224, 3))\n","    input_2 = Input(shape=(224, 224, 3))\n","\n","    base_model = VGGFace(model='resnet50', include_top=False)\n","\n","    for layer in base_model.layers[:-3]:\n","        layer.trainable = True\n","\n","    x1 = base_model(input_1)\n","    x2 = base_model(input_2)\n","\n","\n","    merged_add = Add()([x1, x2])\n","    merged_sub = Subtract()([x1,x2])\n","    \n","    merged_add = Conv2D(100 , [1,1] )(merged_add)\n","    merged_sub = Conv2D(100 , [1,1] )(merged_sub)\n","    \n","    merged = Concatenate(axis=-1)([merged_add, merged_sub])\n","    \n","    merged = Flatten()(merged)\n","\n","    merged = Dense(100, activation=\"relu\")(merged)\n","    merged = Dropout(0.3)(merged)\n","    merged = Dense(25, activation=\"relu\")(merged)\n","    merged = Dropout(0.3)(merged)\n","    out = Dense(1, activation=\"sigmoid\")(merged)\n","\n","    model = Model([input_1, input_2], out)\n","\n","    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n","\n","    model.summary()\n","\n","    return model\n","\n","\n","file_path = \"vgg_face.h5\"\n","\n","checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","\n","reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n","\n","callbacks_list = [checkpoint, reduce_on_plateau]\n","\n","model = baseline_model()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            (None, 224, 224, 3)  0                                            \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            (None, 224, 224, 3)  0                                            \n","__________________________________________________________________________________________________\n","vggface_resnet50 (Model)        multiple             23561152    input_4[0][0]                    \n","                                                                 input_5[0][0]                    \n","__________________________________________________________________________________________________\n","add_34 (Add)                    (None, 1, 1, 2048)   0           vggface_resnet50[1][0]           \n","                                                                 vggface_resnet50[2][0]           \n","__________________________________________________________________________________________________\n","subtract_2 (Subtract)           (None, 1, 1, 2048)   0           vggface_resnet50[1][0]           \n","                                                                 vggface_resnet50[2][0]           \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 1, 1, 100)    204900      add_34[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 1, 1, 100)    204900      subtract_2[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 1, 1, 200)    0           conv2d_3[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 200)          0           concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 100)          20100       flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 100)          0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 25)           2525        dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 25)           0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 1)            26          dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 23,993,603\n","Trainable params: 23,940,483\n","Non-trainable params: 53,120\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"73F7GJss52sx","colab_type":"code","outputId":"dad4e57e-6473-4b10-cd4f-8dac837af869","executionInfo":{"status":"error","timestamp":1561725952794,"user_tz":-330,"elapsed":10745200,"user":{"displayName":"Harish Vadlamani","photoUrl":"https://lh4.googleusercontent.com/-5dWVorerUgM/AAAAAAAAAAI/AAAAAAAACgE/_nrdmzbDU2Y/s64/photo.jpg","userId":"09656886043427580357"}},"colab":{"base_uri":"https://localhost:8080/","height":507}},"source":["model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n","                    validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=10, verbose=1,\n","                    workers = 4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=100)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n","  UserWarning('Using a generator with `use_multiprocessing=True`'\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","200/200 [==============================] - 229s 1s/step - loss: 1.2658 - acc: 0.5616 - val_loss: 0.7927 - val_acc: 0.6231\n","\n","Epoch 00001: val_acc improved from -inf to 0.62313, saving model to vgg_face.h5\n","Epoch 2/10\n","200/200 [==============================] - 204s 1s/step - loss: 0.8433 - acc: 0.5816 - val_loss: 0.6775 - val_acc: 0.6169\n","\n","Epoch 00002: val_acc did not improve from 0.62313\n","Epoch 3/10\n","200/200 [==============================] - 203s 1s/step - loss: 0.7380 - acc: 0.5931 - val_loss: 0.6549 - val_acc: 0.6331\n","\n","Epoch 00003: val_acc improved from 0.62313 to 0.63313, saving model to vgg_face.h5\n","Epoch 4/10\n","200/200 [==============================] - 204s 1s/step - loss: 0.6906 - acc: 0.6069 - val_loss: 0.6341 - val_acc: 0.6362\n","\n","Epoch 00004: val_acc improved from 0.63313 to 0.63625, saving model to vgg_face.h5\n","Epoch 5/10\n","200/200 [==============================] - 204s 1s/step - loss: 0.6575 - acc: 0.6266 - val_loss: 0.6303 - val_acc: 0.6450\n","\n","Epoch 00005: val_acc improved from 0.63625 to 0.64500, saving model to vgg_face.h5\n","Epoch 6/10\n","200/200 [==============================] - 203s 1s/step - loss: 0.6439 - acc: 0.6272 - val_loss: 0.6349 - val_acc: 0.6344\n","\n","Epoch 00006: val_acc did not improve from 0.64500\n","Epoch 7/10\n","197/200 [============================>.] - ETA: 2s - loss: 0.6181 - acc: 0.6526"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sM79WE3l52sz","colab_type":"code","colab":{}},"source":["test_path = root_path + \"/test/\"\n","\n","\n","def chunker(seq, size=32):\n","    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n","\n","\n","from tqdm import tqdm\n","\n","submission = pd.read_csv(root_path + '/sample_submission.csv')\n","\n","predictions = []\n","\n","for batch in tqdm(chunker(submission.img_pair.values)):\n","    X1 = [x.split(\"-\")[0] for x in batch]\n","    X1 = np.array([read_img(test_path + x) for x in X1])\n","\n","    X2 = [x.split(\"-\")[1] for x in batch]\n","    X2 = np.array([read_img(test_path + x) for x in X2])\n","\n","    pred = model.predict([X1, X2]).ravel().tolist()\n","    predictions += pred\n","\n","submission['is_related'] = predictions\n","\n","submission.to_csv(\"vgg_face.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"njrRZXI7kda1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}